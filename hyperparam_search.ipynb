{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Searching of Classifier Hyperparameters\n",
        "\n",
        "#### Introduction\n",
        "\n",
        "One baseline dataset [dimension = 128, random seed = None] was used for searching of optimal classifer hyperparmeters.\n",
        "\n",
        "Two classifiers were experiemented in focus, namely Bagging SVM, Random Forest. \n",
        "\n",
        "The data are normalised. The dataset is split into training and testing set. GridSearchCV is employed to build the classifier pipeline and cross validation for each set of hyperparamters. Three scorers, top20, top100 and AUPRC are used to evaluate the classifiers. The results are used for selection of the best performers. \n",
        "\n",
        "User can use this notebook to amend the hyperparameters as well as the scorer.\n",
        "\n",
        "#### Evaluation of model performance:\n",
        "\n",
        "Input:\tdataset, hyperparameters grid, performance scorers\n",
        "\n",
        "Process:\n",
        "\n",
        "-  Configuration scorers\n",
        "-  Split the baseline dataset into training and testing dataset, \n",
        "-  Configure GridsearchCV pipeline\n",
        "-  Fitting of training data\n",
        "-  display the results\n",
        "\n",
        "Quality control:\n",
        "\n",
        "-\tverify total number of CV splits and the total number of combinations of hyperparameter\n",
        "\n",
        "Output:\t\n",
        "\n",
        "-\tEvaluation results of by the GridSearch method\n",
        "\n",
        "Remarks: User can amend the hyperparameters settings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pulearn import BaggingPuClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, average_precision_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# defined performance metrics scorers\n",
        "\n",
        "def auprc_score(y_true, y_pred):\n",
        "    '''\n",
        "    scoring function of AUPRC\n",
        "    parametes:\n",
        "        y_true: pandas series, series of value of the class label\n",
        "        y_pred: numpy array, the predicted probality of positive class\n",
        "    return:\n",
        "        AUPRC value\n",
        "    '''\n",
        "    return average_precision_score(y_true, y_pred)\n",
        "\n",
        "def topk(y_true, y_pred, top_k=100, get_mask=False):\n",
        "    '''\n",
        "    scoring function of top k hit. \n",
        "    sort the prediction probability, from the toppest k predictions, count the numbers of true positives being predicted\n",
        "    parametes:\n",
        "        y_true: pandas series, series of true label\n",
        "        y_pred: numpy array, the predicted probality of positive class\n",
        "        top_k: int, the k value to be set for the scorer for calculating how many hits on the toppest k predictions\n",
        "        get_mask: bool, to control the function to export number of hits or the positive mask of the top k predictions\n",
        "    return:\n",
        "        if get_mask is false: return top k hit score\n",
        "        if get_mask is true: return the positve mask \n",
        "    '''\n",
        "    sorted_indices = y_pred.argsort()[::-1]\n",
        "    top_k_indices = sorted_indices[:top_k]\n",
        "    y_pred_top_k_mask = np.full(y_true.shape, False, dtype=bool)\n",
        "    y_pred_top_k_mask[top_k_indices] = True\n",
        "    top_k_hits = y_true.values[y_pred_top_k_mask].sum()\n",
        "    \n",
        "    if get_mask == False:\n",
        "        return top_k_hits\n",
        "    else: \n",
        "        return y_pred_top_k_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load baseline dataset (d=128, random seed = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the subject dataset\n",
        "\n",
        "dataset_filename = 'dataset_p_4_q_1_dim_128_walkleng_100_numwalks_500.csv'\n",
        "\n",
        "file_path = os.path.join('data', 'datasets', dataset_filename)\n",
        "dataset = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Splitting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset shape, X: (12968, 128)\n",
            "Training dataset shape, y: (12968,)\n",
            "Testing dataset shape, X: (3242, 128)\n",
            "Testing dataset shape, y: (3242,)\n"
          ]
        }
      ],
      "source": [
        "# X is the data of feature1 to feature128 in the dataset\n",
        "X = dataset.drop(['id', 'y'], axis=1)\n",
        "# y is the target value of the last column in the dataset\n",
        "y = dataset['y']\n",
        "\n",
        "# split the dataset into training and testing with 80% to 20% proportion and randomly shuffle the data\n",
        "train_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.2, random_state=37,stratify=y)\n",
        "\n",
        "train_indices = train_X.index\n",
        "test_indices = test_X.index\n",
        "\n",
        "# print the shape of the data\n",
        "print(f'Training dataset shape, X: {train_X.shape}')\n",
        "print(f'Training dataset shape, y: {train_y.shape}')\n",
        "print(f'Testing dataset shape, X: {test_X.shape}')\n",
        "print(f'Testing dataset shape, y: {test_y.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Search for Bagging SVM parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalise the feature data and build Bagging SVM classifer pipline\n",
        "\n",
        "scaler = StandardScaler()\n",
        "base_clf = SVC()\n",
        "clf = BaggingPuClassifier(base_estimator=base_clf, n_jobs = -1, random_state=44, verbose=0)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scale\", scaler),\n",
        "    (\"clf\", clf)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup hyperparmeter grid by phases, broader search of key parameters, such as C\n",
        "\n",
        "pu_estimator = GridSearchCV(estimator=pipe, \n",
        "                            param_grid={\n",
        "                                        'clf__base_estimator__C':[1, 2, 3, 4],\n",
        "                                        'clf__n_estimators':[200],\n",
        "                                        'clf__max_samples': [500, 700]\n",
        "                                        },\n",
        "                            scoring={\n",
        "                                     'top20k': make_scorer(topk, greater_is_better=True, needs_proba=True, top_k=20),\n",
        "                                     'top100k': make_scorer(topk, greater_is_better=True, needs_proba=True, top_k=100),\n",
        "                                     'auprc_scorer' : make_scorer(auprc_score, needs_proba=True)},                                     \n",
        "                            refit='auprc_scorer',\n",
        "                            return_train_score=True,\n",
        "                            cv=3\n",
        "                            )\n",
        "pu_estimator.fit(train_X, train_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "cv_results = pd.DataFrame(pu_estimator.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set the display limit to be shown in this notebook\n",
        "pd.set_option('display.max_columns', 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show cv_results\n",
        "cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# narrow down some best hyperparameters and search of other hyperparameters\n",
        "\n",
        "pu_estimator = GridSearchCV(estimator=pipe, \n",
        "                            param_grid={\n",
        "                                        'clf__max_features' : [0.5, 0.75, 1.0],\n",
        "                                        'clf__base_estimator__C':[3, 4],\n",
        "                                        'clf__n_estimators':[100, 200],\n",
        "                                        'clf__max_samples': [500, 600], \n",
        "                                        },\n",
        "                            scoring={\n",
        "                                     'top20k': make_scorer(topk, greater_is_better=True, needs_proba=True, top_k=20),\n",
        "                                     'top100k': make_scorer(topk, greater_is_better=True, needs_proba=True, top_k=100),\n",
        "                                     'auprc_scorer' : make_scorer(auprc_score, needs_proba=True)},                                     \n",
        "                            refit='auprc_scorer',\n",
        "                            return_train_score=True,\n",
        "                            cv=3\n",
        "                            )\n",
        "pu_estimator.fit(train_X, train_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_results = pd.DataFrame(pu_estimator.cv_results_)\n",
        "pd.set_option('display.max_columns', 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Search for hyperparameters of Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# build Random Forest classifier pipeline\n",
        "\n",
        "scaler = StandardScaler()\n",
        "base_clf = DecisionTreeClassifier()\n",
        "clf = BaggingPuClassifier(base_estimator=base_clf, n_jobs = -1, random_state=44, verbose=0)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scale\", scaler),\n",
        "    (\"clf\", clf)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# perform comprehensive search of the hyperparamters. This would take some time \n",
        "# as there are over 4000 set of hyperparmeter combinations.\n",
        "\n",
        "pu_estimator = GridSearchCV(estimator=pipe, \n",
        "                            param_grid={\n",
        "                                        'clf__bootstrap_features': [True, False],\n",
        "                                        'clf__base_estimator__max_leaf_nodes': [100, 120, 140],\n",
        "                                        'clf__base_estimator__max_depth': [12, 17, 22],\n",
        "                                        'clf__base_estimator__min_samples_leaf': [1, 2, 4],\n",
        "                                        'clf__base_estimator__min_samples_split': [2, 5, 10],\n",
        "                                        'clf__max_features' : [0.5, 0.75, 1.0],\n",
        "                                        'clf__n_estimators':[100, 200, 300],\n",
        "                                        'clf__max_samples': [500, 600, 700], \n",
        "                                        },\n",
        "                            scoring={\n",
        "                                     'top20k': make_scorer(topk, greater_is_better=True, needs_proba=True, top_k=20),\n",
        "                                     'top100k': make_scorer(topk, greater_is_better=True, needs_proba=True, top_k=100),\n",
        "                                     'auprc_scorer' : make_scorer(auprc_score, needs_proba=True)},                                     \n",
        "                            refit='auprc_scorer',\n",
        "                            return_train_score=True,\n",
        "                            cv=3\n",
        "                            )\n",
        "pu_estimator.fit(train_X, train_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_results = pd.DataFrame(pu_estimator.cv_results_)\n",
        "pd.set_option('display.max_columns', 80)\n",
        "cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# due to the size of this search is enormous, a tool is built to summaries the results\n",
        "\n",
        "cv_results_summary = cv_results.copy()\n",
        "\n",
        "col_list = [                                       \n",
        "    'param_clf__bootstrap_features',\n",
        "    'param_clf__base_estimator__max_leaf_nodes',\n",
        "    'param_clf__base_estimator__max_depth',\n",
        "    'param_clf__base_estimator__min_samples_leaf',\n",
        "    'param_clf__base_estimator__min_samples_split',\n",
        "    'param_clf__max_features',\n",
        "    'param_clf__n_estimators',\n",
        "    'param_clf__max_samples'\n",
        "]\n",
        "\n",
        "cat_summaries = []\n",
        "\n",
        "for col_name in col_list:\n",
        "        cat_summary = cv_results_summary.groupby(col_name).agg({'mean_test_top20k': 'mean', 'rank_test_top20k': 'mean', 'mean_test_top100k': 'mean', 'rank_test_top100k': 'mean', 'mean_test_auprc_scorer':'mean', 'rank_test_auprc_scorer' : 'mean'}).reset_index()\n",
        "        cat_summary.rename(columns={col_name: 'parameters'}, inplace=True)\n",
        "        cat_summary.insert(0, 'param_name', col_name)\n",
        "        cat_summaries.append(cat_summary)\n",
        "\n",
        "stacked_df = pd.concat(cat_summaries, axis=0, ignore_index=True)\n",
        "stacked_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set constants or narrow down the range of certain parameters, and perform a second round of search\n",
        "\n",
        "pu_estimator = GridSearchCV(estimator=pipe, \n",
        "                            param_grid={\n",
        "                                        'clf__base_estimator__max_leaf_nodes': [120],\n",
        "                                        'clf__base_estimator__max_depth': [8, 10, 12],\n",
        "                                        'clf__base_estimator__min_samples_leaf': [2, 3, 4],\n",
        "                                        'clf__base_estimator__min_samples_split': [2],\n",
        "                                        'clf__n_estimators':[200],\n",
        "                                        'clf__max_samples': [500, 600], \n",
        "                                        },\n",
        "                            scoring={\n",
        "                                     'top20k': make_scorer(topk, greater_is_better=True, needs_proba=True, top_k=20),\n",
        "                                     'top100k': make_scorer(topk, greater_is_better=True, needs_proba=True, top_k=100),\n",
        "                                     'auprc_scorer' : make_scorer(auprc_score, needs_proba=True)},                                     \n",
        "                            refit='auprc_scorer',\n",
        "                            return_train_score=False,\n",
        "                            cv=3\n",
        "                            )\n",
        "pu_estimator.fit(train_X, train_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_results = pd.DataFrame(pu_estimator.cv_results_)\n",
        "pd.set_option('display.max_columns', 80)\n",
        "cv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_results_summary = cv_results.copy()\n",
        "\n",
        "col_list = [                                       \n",
        "    'param_clf__base_estimator__max_depth',\n",
        "    'param_clf__base_estimator__min_samples_leaf',\n",
        "    'param_clf__base_estimator__min_samples_split',\n",
        "    'param_clf__max_samples'\n",
        "]\n",
        "\n",
        "cat_summaries = []\n",
        "\n",
        "for col_name in col_list:\n",
        "        cat_summary = cv_results_summary.groupby(col_name).agg({'mean_test_top20k': 'mean', 'rank_test_top20k': 'mean', 'mean_test_top100k': 'mean', 'rank_test_top100k': 'mean', 'mean_test_auprc_scorer':'mean', 'rank_test_auprc_scorer' : 'mean'}).reset_index()\n",
        "        cat_summary.rename(columns={col_name: 'parameters'}, inplace=True)\n",
        "        cat_summary.insert(0, 'param_name', col_name)\n",
        "        cat_summaries.append(cat_summary)\n",
        "\n",
        "stacked_df = pd.concat(cat_summaries, axis=0, ignore_index=True)\n",
        "stacked_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
